---
title: "Exploratory Data Analysis and Visualisation G (11517)"
subtitle: "Final Project"
author: ""
date: "Semester 1 2021"
output:
  pdf_document:
    highlight: breezedark
    df_print: kable
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# A Study into Factors People Considered on House Price

## 1. Problem identification

This is a dataset regarding residential homes in Ames, Iowa, which compiled by Dean De Cock. It comes with 79 explanatory variables for each house covering the data from number of rooms to quality of pools. Total number of columns is 81, of which 43 is categorical variables, 36 is quantitative variables, and ID as well as house price. There are 1460 entries each in train set and test set.

Problems:

* Identify `HouseStyle` with highest average price and growth trend;
* Analyze possible pattern of `SalePrice` vs `MoSold`;
* Analyze if condition of the land(`LotShape`, `LandContour`) affect the growth potential;
* Analyze people prefer more `Bedroom` or larger land(`LotArea`);
* Analyze if the zoning classification(`MSZoning`) affect the price;
* Analyze if pool(`PoolQC`) has significant impact on house price;
* Use the model to identify the most promising `Neighborhood` in terms of growth.

## 2. Data preprocessing

For any dataset or raw data collected from the wild, before we can explore the data within, we need to clean the data, which include identifying missing or error values, identifying possible outliers and checking the structure of the dataset.

In this session, through data preprocessing, try to make sure each column stores same variables with consistent data type and no invalid values.
```{r include = FALSE}
# loading required libraries
require(tidyverse)
require(ggplot2)
require(GGally)
require(DMwR2)

# load the data from csv file
trainer <- read_csv("train.csv")
tester <- read_csv("test.csv")
```

```{r echo = FALSE}
# count the NAs in train dataset
missing <- colSums(is.na(trainer))

# plot the percentage of NAs for the involved variables
missing %>%
    tibble(rownames = names(.)) %>%
    rename(., na = .) %>%
    filter(na > 0) %>%
    mutate(na = round(na / nrow(trainer) * 100, digit = 2)) %>%
    ggplot(aes(rownames, na, label = na)) +
        ggtitle("Missing value ratio of Variables") +
        ylab("NAs (%)") +
        geom_col(aes(fill = na), show.legend = F) +
        geom_text(nudge_y = 3) +
        scale_fill_viridis_c() +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"),
              axis.text.x = element_text(angle = 45, vjust = 0.5),
              axis.title.x = element_blank())
```
There are 19 variables have missing values. 4 of them over 50%. Those NAs are missing features of the house such as alley access or fence. They are not error values so should not imput.

```{r echo = FALSE}
# use boxplot stats to find possible outliers
trainer %>%
    .[which(.$SalePrice %in% boxplot.stats(.$SalePrice)$out), ] %>%
    group_by(Neighborhood) %>%
    summarize(count = n()) %>%
    arrange(desc(count))
```
Amoung the outliers, nearly half of them comes from the neiborhood NridgHt.

```{r echo = FALSE}
trainer %>%
    .[which(.$SalePrice %in% boxplot.stats(.$SalePrice)$out), ] %>%
    group_by(YrSold) %>%
    summarize(count = n()) %>%
    arrange(desc(count))
```
Out of the 61 outliers of sale price, they have no common in year of sale.

At this stage, there is no need to remove these outliers, they could just represent particular category. 


## 3. EDA

Plot correlation matrix of numerical variables.

```{r echo = FALSE}
# plot correlation matrix for all numeric variables
trainer %>%
    select_if(is.numeric) %>%
    select(-Id) %>%
    ggcorr()
```

Use correlation matrix to find variables which have significant impact on `SalePrice` for further analysis.
```{r echo = FALSE}
# keep the variables with high correlation coefficient to SalePrice
num_predictor <- trainer %>%
    select_if(is.numeric) %>%
    select(names(which(colSums(is.na(.)) == 0))) %>%
    select(-Id) %>%
    cor() %>%
    .[, "SalePrice"] %>%
    abs() %>%
    as.data.frame() %>%
    filter(. > 0.5) %>%
    arrange(desc(.))

num_predictor
```

Plot the distribution of these variables.

```{r echo = FALSE}
# plot the distributions of these variables
trainer %>%
    select(all_of(rownames(num_predictor)), -SalePrice) %>%
    gather() %>%
    ggplot(aes(value)) +
        geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 50) +
        geom_density(colour = "darkred") +
        ggtitle("Distribution of variables with high r") +
        xlab("Variables") +
        facet_wrap(~ key, scales = "free") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"),
              axis.title.y = element_blank())
```
We can see some of them have shape close to normal distribution which is suitable for linear model.

```{r echo = FALSE}
# show scatter plot of these variables
trainer %>%
    select(all_of(rownames(num_predictor))) %>%
    gather("column", "value", -SalePrice) %>%
    ggplot(aes(value, SalePrice)) +
        geom_point() +
        geom_smooth(method = "lm") +
        ggtitle("Scatter plot of variables with high r") +
        xlab("Variables") +
        facet_wrap(~ column, scales = "free") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"),
              axis.title.y = element_blank())
```
5 of them are promising:
  `OverallQual`, `GrLivArea`, `GarageArea`, `TotalBsmtSF`, `1stFlrSF`.

Generate boxplots for categorical variables of interest.

```{r echo = FALSE}
interest_var <- c("Neighborhood", "MSZoning", "PoolQC", "LotShape", "LandContour")
trainer %>%
    select(interest_var, SalePrice) %>%
    gather("column", "value", -SalePrice) %>%
    ggplot(aes(value, SalePrice)) +
        geom_boxplot(aes(fill = value), show.legend = F) +
        scale_fill_viridis_d() +
        ggtitle("Distribution of price between categories") +
        xlab("Category") +
        ylab("Price") +
        facet_wrap(~ column, scales = "free") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```
Look at the boxplots of categorical variables, some of the categories have very different location than others, which means they might have huge impact on price, like `Neiborhood`, `ExterQual` or `PoolQC`

To evaluate the impact of these categorical variables on SalePrice,
convert them based on mean price of each category
```{r}
score <- trainer %>%
    select(where(is.character), SalePrice) %>%
    gather("column", "value", -SalePrice) %>%
    group_by(column, value) %>%
    summarize(level = mean(SalePrice))

# convert them to z-score
score$level <- scale(score$level)
```

```{r}
# generate a new data frame to evaluate these categorical variables
cat_var <- trainer %>% select(where(is.character))

# put the converted variables in
for (item in names(cat_var)) {
    value <- score %>% filter(column == item)
    colnames(value)[2] <- item
    cat_var <- cat_var %>% inner_join(value) %>% mutate(column = NULL)
    colnames(cat_var)[dim(cat_var)[2]] <- paste0(item, "_level")
}

# remove old data, put the SalePrice back
cat_var <- cat_var %>%
    select(where(is.numeric)) %>%
    mutate(SalePrice = trainer$SalePrice)

# visualize the correlation matrix
ggcorr(cat_var)
```

```{r echo = FALSE}
# keep the variables with high correlation coefficient
cat_predictor <- cat_var %>%
    cor() %>%
    .[, "SalePrice"] %>%
    abs() %>%
    as.data.frame() %>%
    filter(. > 0.5) %>%
    arrange(desc(.))

cat_predictor
```
keep the variables with high correlation coefficient.

```{r include = FALSE}
# append the converted level data to training set
trainer <- trainer %>%
    mutate(SalePrice = NULL) %>%
    cbind(cat_var)
```


## 4. Further preprocessing

```{r echo = FALSE}
trainer %>%
    select(all_of(rownames(num_predictor))) %>%
    ggcorr(label = T)
```
We can see there are variables related to each other regarding garage, basement and 1st floor area, so no need to take them all or it can lead to over fitting.

```{r echo = FALSE}
trainer %>%
    select(all_of(rownames(cat_predictor))) %>%
    ggcorr(label = T)
```
Similar here we only need the most important two with less relation to other variables.

```{r echo = FALSE}
model_var <- c("OverallQual", "GrLivArea",
               "GarageArea", "TotalBsmtSF",
               "Neighborhood_level", "ExterQual_level")

model_var
```

Here we will remove the small number of the houses without garage or basement
because we use garage and basement as predictors
```{r include = FALSE}
trainer <- trainer %>%
    filter(GarageArea > 0) %>%
    filter(TotalBsmtSF > 0)

tester <- tester %>%
    filter(GarageArea > 0) %>%
    filter(TotalBsmtSF > 0)
```

Find outliers based on these variables and remove them for better fitting
```{r}
# identify possible outliers based on these variables
num_out <- trainer %>%
    select(all_of(model_var)) %>%
    scale() %>%
    lofactor(5)

num_out <- which(num_out > 1.5)

trainer <- trainer %>% slice(-num_out)
```

Plot the final variables selected.

```{r echo = FALSE}
# plot the distributions of these variables
trainer %>%
    select(all_of(model_var)) %>%
    gather() %>%
    ggplot(aes(value)) +
        geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 50) +
        geom_density(colour = "darkred") +
        ggtitle("Distribution of variables with high r") +
        xlab("Variables") +
        facet_wrap(~ key, scales = "free") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"),
              axis.title.y = element_blank())
```


```{r echo = FALSE}
# show scatter plot of these variables
trainer %>%
    select(all_of(model_var), SalePrice) %>%
    gather("column", "value", -SalePrice) %>%
    ggplot(aes(value, SalePrice)) +
        geom_point() +
        geom_smooth(method = "lm") +
        ggtitle("Scatter plot of variables with high r") +
        xlab("Variables") +
        facet_wrap(~ column, scales = "free") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"),
              axis.title.y = element_blank())
```


```{r include = FALSE}
# convert selected categorical variables in test set
var <- c("Neighborhood", "ExterQual")

# put the coverted variables in test set
for (item in var) {
    value <- score %>% filter(column == item)
    colnames(value)[2] <- item
    tester <- tester %>% inner_join(value) %>% mutate(column = NULL)
    colnames(tester)[dim(tester)[2]] <- paste0(item, "_level")
}
```

```{r echo = FALSE}
# plot the distribution of SalePrice
ggplot(trainer, aes(SalePrice)) +
    geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 100) +
    geom_density(colour = "darkred") +
    ggtitle("Distribution of SalePrice") +
    xlab("SalePrice") +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          axis.title.y = element_blank())
```

It is clear that SalePrice does not follow normal distribution. Try to apply log trasformation.
```{r echo = FALSE}
ggplot(trainer, aes(log(SalePrice))) +
    geom_histogram(aes(y = ..density..), fill = "lightblue", bins = 100) +
    geom_density(colour = "darkred") +
    ggtitle("Distribution of SalePrice (logarithmic)") +
    xlab("SalePrice") +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          axis.title.y = element_blank())

# transforms the scale of SalePrice to log for the trainer set and tester set
trainer <- trainer %>% mutate(SalePrice = log(SalePrice))
tester <- tester %>% mutate(SalePrice = log(SalePrice))
```
Now `SalePrice` follows normal distribution better. 


## 5. Modelling

First model uses top 4 numerical variables
```{r echo = FALSE}
# Model 1
model_1 <- lm(SalePrice ~ GrLivArea +
                          OverallQual +
                          GarageArea +
                          TotalBsmtSF, data = trainer)
tester$predict_1 <- predict(model_1, tester)
summary(model_1)
```

Second model uses top 2 numerical and top 2 converted categorical variables
```{r echo = FALSE}
# Model 2
model_2 <- lm(SalePrice ~ GrLivArea +
                          OverallQual +
                          ExterQual_level +
                          Neighborhood_level, data = trainer)
tester$predict_2 <- predict(model_2, tester)
summary(model_2)
```

Both model shows R squre of over 80% which are good linear model.


## 6. Evaluation

Convert the scale of price back
```{r}
# convert the price back to original scale
tester <- tester %>% mutate(SalePrice = exp(SalePrice))
tester <- tester %>% mutate(predict_1 = exp(predict_1))
tester <- tester %>% mutate(predict_2 = exp(predict_2))
```

```{r}
# rmse of model 1
cat("RMSE of model 1:")
sqrt(mean((tester$predict_1 - tester$SalePrice)^2))

# rmse of model 2
cat("RMSE of model 1:")
sqrt(mean((tester$predict_2 - tester$SalePrice)^2))
```
With similar R square value, the second model has lower RMSE



Generate the residuals plot
```{r include = FALSE}
# residuals plot
lm_result <- tester %>%
    filter(predict_1 < 6e+05) %>%
    filter(predict_2 < 6e+05) %>%
    mutate(diff_1 = abs(SalePrice - predict_1) / SalePrice) %>%
    mutate(diff_2 = abs(SalePrice - predict_2) / SalePrice)

bad_margain <- 0.15

bad_result_1 <- lm_result %>% filter(diff_1 > bad_margain)
bad_result_2 <- lm_result %>% filter(diff_2 > bad_margain)
```

```{r echo = FALSE}
lm_result %>%
    ggplot(aes(SalePrice, predict_1)) +
        geom_point(aes(col = diff_1)) +
        geom_point(data = bad_result_1, col = "red") +
        scale_color_gradient(name = "y - yBar (%)", limits = c(0, 50)) +
        geom_abline(slope = 1, intercept = 0) +
        ggtitle("Linear model 1 residuals") +
        xlab("Price") +
        ylab("Prediction") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

```{r echo = FALSE}
lm_result %>%
    ggplot(aes(SalePrice, predict_2)) +
        geom_point(aes(col = diff_2)) +
        geom_point(data = bad_result_2, col = "red") +
        scale_color_gradient(name = "y - yBar (%)", limits = c(0, 50)) +
        geom_abline(slope = 1, intercept = 0) +
        ggtitle("Linear model 2 residuals") +
        xlab("Price") +
        ylab("Prediction") +
        theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```
From the residuals plots, we can see both model fit well, while model 2 can be a little bit better.


## 7. Recommendations and final conclusions

Clearly, different people have different aspects of interest on house. In this study, other than the obvious location(`Neighborhood`), pool, garage and basement all have big impact on house price. But, on the other hand, land(`LotShape`, `LandContour`) show no significant impact.

For the second model, better RMSE comes from the two categorical variables which have higher correlation coefficient while not in better shape of distribution.

For further improvement, we could introduce multiple model for different categories so that within the categories the fitting can be better and than the whole prediction can be better.

## 8. References

* Dean De Cock, House Prices - Advanced Regression Techniques [Online]. Available at: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview (Accessed: 7 May 2021)